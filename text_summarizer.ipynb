{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPoT4AEAesZt5COGl8D+5u/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elorie-bernard-lacroix/Elorie_Bernard-Lacroix/blob/main/text_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDcmaeb9_81U"
      },
      "outputs": [],
      "source": [
        "#import necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#examine the data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(\"tennis.csv\")\n",
        "df.head()\n",
        "\n",
        "df['article_text'][1]\n",
        "\n",
        "#split the sequences into the data by tokeizing them using a list\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = []\n",
        "for s in df['article_text']:\n",
        "    sentences.append(sent_tokenize(s))\n",
        "\n",
        "sentences = [y for x in sentences for y in x]\n",
        "\n",
        "#using the glove method for word representation (generates word integrations by aggregating the global word-to-word co-occurence matrix from a corpus)\n",
        "word_embeddings = {}\n",
        "f = open('/content/glove.6B.100d[1].txt', encoding='utf-8')\n",
        "#try:\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "#except:\n",
        "  #print(\"An exception occurred\")\n",
        "\n",
        "f.close()\n",
        "\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "clean_sentences = [s.lower() for s in clean_sentences]\n",
        "stop_words = stopwords.words('english')\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
        "\n",
        "#creating vectors for the sentences\n",
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors.append(v)\n",
        "\n",
        "#finding similarities to summarize the text\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
        "\n",
        "#converting sim_mat similiarity matrix into the graph (nodes = sentences, edges = similarity scores)\n",
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)\n",
        "\n",
        "#summarizing text\n",
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "for i in range(5):\n",
        "  print(\"ARTICLE:\")\n",
        "  print(df['article_text'][i])\n",
        "  print('\\n')\n",
        "  print(\"SUMMARY:\")\n",
        "  print(ranked_sentences[i][1])\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "YZDpYR47Amkm"
      }
    }
  ]
}